--------------
DONE
-------------

1) Ver como se realiza el finetunning/extract features en un modelo de red:
	- Extract features:
	    Se corta una red pre-entrenada en cualquier punto (normalmente se excluyen las capas FC).
		Se utiliza esta red para pasar las imagenes en el proceso de feed forward. En la capa recortada, se recupera
		el vector de características y se le aplica un flaten.
		Se utiliza el vector para entrenar un modelo:
			- Si el set de datos es pequeño y cabe en memoria se pueden utilizar SVMs o Linear Regresion.
			- Si el set de datos es grande y no cabe en memoria se realiza un algorítmo de entrenamiento adaptativo,
			  es decir, que se pueda entrenar por batches. En resumen, una CNN. Por lo tanto realizar un
			  extract features consistirá en congelar los pesos de las capas convolucionales y entrenar el nuevo modelo
			  de red creado a la salida.
   - Fine Tuning:
   		Se cortan las capas FC de una red pre-entrenada.
		Se añade una nueva arquitectura de capas FC sobre las capas convolucionales de la red. Estas capas FC nuevas
		tendrán pesos aleatorios de forma que, si se entrena la red de forma completa, con la actualización de los pesos
		de las capas Convolucionales se podrían perder las características entrenadas -> No se quiere esto. De modo
		que es habitual utilizar una estrategia de warm up para las capas FC (se congela la parte CNN de la red y
		se entrenan los pesos de las nuevas capas FC con un learning rate bajo).
		Para incrementar la accuracy, se pueden descongelar las capas CNN y reentrenar el modelo con un learning rate de
		nuevo muy bajo.

2) Crear en model la congelación de las capas
	- Ver con el include_top = False qué capas contiene cada CNN y modificar las capas adicionales del baseline.
	- Al añadir las capas nuevas, configurar el número de neuronas de las NN con la regla del pulgar:
		"A good rule of thumb is to take the square root of the previous number of nodes in the layer and then find the
	 	closest power of 2."

3) Crear en model.py los optimizadores y el learning rate utilizado para cada etapa: SGD y Adam con learning rates del
   paper.
   - Ver en callbacks lo del decay para los optimizadores.

5) Crear el pipeline de entrenar el modelo con finetunning y train from scratch con pesos aleatorios.
    - 5.1: Random Initialization: Se crea la arquitectura con los pesos entrenados de forma random y se entrena
                                  todo el modelo (unfreeze de todas las capas).
    - 5.2: Feature Extraction: Se congelan todas las capas convolucionales y se entrenan exclusivamente las capas FC.
                               (0 - fine tunning estrategy). Se utilizará un optimizador Adam durante 30 epocas.
    - 5.3: Fine Tunning: Se ponen como trainable de 1 - N capas convolucionales del modelo realizando dos fases:
                        1) Warm up: Se entrena únicamente la parte FC con optimizador Adam y 30 épocas
                        2) FineTunning: Se entrenan las capas convolucionales que aparecen como unfreeze con un
                                        optimizador SGD y un learning rate de 10-4 con weight decay de 10 y
                                        early stopping con paciencia de 20. El entreno se realiza durante 90 epocas.
6) Preprocesado de imagenes
      6.1 - Crear un modulo que permita distintos procesados de las imagenes almacenando su configuración:

7) Log con el callback end_train que permita:
    - Recoger el tiempo de train
    - # Parámetros entrenados
    - # Número de capas unfreeze

8) Arreglar el model ensambling

9) Visualizaciones:
    9.1 - Gráfico de métricas registradas durante el entrenamiento (AUC, precision, loss, recall, accuracy, f1)
          Los gráficos muestran las distintas redes y las fases de train/val.
          Se crea un fichero para cada inicializacion de pesos/finnetunning-extractfeatures-all/capas congeladas.
    9.2 - Gráfico con el tiempo de entrenamiento de los modelos.
          Barplot con hue=frozen_layers, x=Redes, y=suma tiempo entrenamiento (FT+EF vs Scratch), col=weights
    9.5 - Gráfico para datos de train donde se muestra la matriz de confusión de cada red + gradient boosting.
          Se crea un gráfico para cada combinación de weighs, layers
    9.6 - Idem 9.5 con datos de validación.
    9.7 - Models métrics para cada combinaciónd e weiths, layers.
    9.3 - Gráfico de barras que muestra la accuracy de imagenet vs random utilizando el train all.
          Barplot con hue=weights, x=redes,y=accuracy, data=data[layers == 'all']
    9.4 - Gráfico de barras según accuracy en imagenet en función de la red
          Barplot con hue=layers, x=redes, y=accuracy, data=data[weighs=='imagenet']

11) FUNCION QUE CREA LA MASCARA DE LAS IMAGENES Y LAS ALMACENA EN LA CARPETA 02_PROCESED\MASK

    11.1) DATASET CBIS
            -> LAS MASCARAS SE DEBEN DE LEER DEL CSV Y LINCAR LA PATOLOGÍA CON EL ID (ALERTA CON CAMBIOS DE NOMBRES) Y
               MULTIPLES PATOLOGÍAS PARA UNA MISMA IMAGEN.
            -> LA FUNCIÓN DE CONVERT DEBE DE LEER LAS MASCARA
13) Evitar segregación en train y val de parches de la misma patología con overlap -> Sesgo. (DEPRECATED)
14) **Aplicación de recorte de parche a partir de la máscara obtenida por el modelo de segmentación.** (DEPRECATED)
11) Implementación background y overlap para parches. (DEPRECATED)
17) Revisión DataViz:
        - Los logs de resultados salen con las métricas giradas. (DONE)
        - Unificar extract features y finetunning en la misma gráfica. (DONE)
        - **Añadir métricas IoU y dice para los modelos de segmentación.** (DEPRECATED)
        - Arreglar visualización de data augmentation. (DONE)
        - Añadir el número de epocas en time executions (DONE)

-------
TODO
------
10) En model ensambler leer el csv y no la clase DataBreastCancer para poder hacer distintas pruebas
11) Realizar deployment
12) No hacer resize de los crops
15) Aplicar auto batch-balance.
16) Aplicar regularización L2 con weight decay.
17) Revisión DataViz:
    - Añadir métrica AUC en la tabla de metrics. (NOT POSIBLE -> PROBS NECESSARY)
    - Revisar que las visualizaciones son correctas.
    - Arreglar visualización de steps de processed imgs.
18) Procesado de imagenes.
    - Aplicar Global Contrast Norm como configuración 2 del dataset
    - Testear crops de rois cuadrados


---------
  TESTS
---------
EJEC_ROI_TEST1:
    - Modelo previo al refactor de frozen layers (se excluyen bn en train y se añaden capas en algunos modelos).
    - El data augmentation contenia fill_mode cte con pixeles negros
    - Se utilizan todos los dataset
    - El Crop presenta un margen de 1.2, sin tener background y con un único crop por ROI (NO OVERLAP).
    - En el data augmentation hay funciones como brightness (no tiene sentido al aplicar clahe)
      y zoom que pueden meter ruido al hacer desaparecer las patologías.

    **RESULTADOS**:
    - acc train > 80% para modelo Gradient Boosting con all layers en imagenet
    - acc val   > 70% para modelo Densenet con all layers en imagenet

EJEC_ROI_TEST2
    - Se suprime el dataset MIAS del entrenamiento
    - Se suprime fill mode cte y función brightness del data augmentation manteniendo zoom.
    - Se corrige la función de transformación prévia al paso de preprocesado de cada red (cast a float 32).
    - Post refactor de frozen layers

EJEC_ROI_TEST3:
    - Añadir fully connected layers a la salida del baseline.

EJEC_ROI_TEST4:
    - El resize se hace una única vez y no dos (image preprocessing + data augmentation).

EJEC_ROI_TEST4:
    - Se añade tercera fase de entrenamiento para fine-tunning añadiendo un unfroze all con SGD(1e-5) y
      reducción de learning rate en un factor de 10.

EJEC_ROI_TEST5:
    - Se añade regularización L2

EJEC_ROI_TEST6:
    - Se aumenta el margin de los rois a 1.4.

EJEC_ROI_TEST7:
    - Añadir data augmentation al set de validacion


---- EXPERIMENTOS DEPRECATED ----
EJEC_SEGMENTATION_TEST1: (DEPRECATED)
    - Parches de tamaño 384, 192 y BS = 9

EJEC_SEGMENTATION_TEST2: (DEPRECATED)
    - Parches de tamaño 512, 256 y BS = 5

EJEC_SEGMENTATION_TEST3: (DEPRECATED)
    - Parches de tamaño 1024, 512 y BS = 1

EJEC_SEGMENTATION_TEST4: (DEPRECATED)
    - Parches de tamaño 448, 224 y BS = 10







